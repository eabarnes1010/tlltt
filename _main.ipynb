{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f6a48b-bab3-4333-abf3-07711e94b616",
   "metadata": {},
   "source": [
    "# This Looks Like That in Tensorflow\n",
    "\n",
    "Useful Files and Links\n",
    "- Chen et al. (2019)\n",
    "- https://github.com/cfchen-duke/ProtoPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bf3dd4-d4af-4c8c-89f6-67097628c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import imp #imp.reload(module)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import network\n",
    "import experiment_settings \n",
    "import data_functions\n",
    "import push_prototypes\n",
    "import plots\n",
    "import common_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e0dba3-9a8a-4d45-b00b-7aaface5fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Elizabeth A. Barnes and Randal J Barnes\"\n",
    "__version__ = \"23 November 2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18825d5a-a951-4a29-8dbd-1b1956974b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "mpl.rcParams['figure.dpi']= 150\n",
    "dpiFig = 300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce58c40-dd37-43ea-9ad3-6d6b56b7d166",
   "metadata": {},
   "source": [
    "## Print the detailed system info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e923341-795f-428a-b5bf-d8cafdce1bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version = 3.9.4 (default, Apr  9 2021, 09:32:38) \n",
      "[Clang 10.0.0 ]\n",
      "numpy version = 1.20.1\n",
      "tensorflow version = 2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"python version = {sys.version}\")\n",
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"tensorflow version = {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b515327-ece1-478c-9c99-0d3979739cbe",
   "metadata": {},
   "source": [
    "## Define experiment settings and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db315d0-951f-4fe1-88b7-168343dfe3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'quadrants'\n",
    "\n",
    "imp.reload(experiment_settings)\n",
    "settings = experiment_settings.get_settings(EXP_NAME)\n",
    "\n",
    "imp.reload(common_functions)\n",
    "model_dir, model_diagnostics_dir, vizualization_dir = common_functions.get_exp_directories(EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c333c24-ae7d-43d3-bc87-09fc8e7d8e89",
   "metadata": {},
   "source": [
    "## Define the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3005d411-649b-484a-8c3e-fc23500e11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED          = settings['random_seed']\n",
    "BATCH_SIZE_PREDICT   = settings['batch_size_predict']\n",
    "BATCH_SIZE           = settings['batch_size']\n",
    "NLAYERS              = settings['nlayers']\n",
    "NFILTERS             = settings['nfilters']   \n",
    "assert(len(NFILTERS)==NLAYERS)\n",
    "\n",
    "NCLASSES             = settings['nclasses']\n",
    "PROTOTYPES_PER_CLASS = settings['prototypes_per_class']\n",
    "NPROTOTYPES          = np.sum(PROTOTYPES_PER_CLASS)\n",
    "\n",
    "NEPOCHS              = settings['nepochs']\n",
    "LR_INIT              = settings['lr']\n",
    "LR_CALLBACK_EPOCH    = settings['lr_cb_epoch']\n",
    "PATIENCE             = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab2e28-c92f-4c2d-a5d2-96dcfd9625a3",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb857807-8efc-485b-8be6-29751f6a4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a7140-1796-429f-ad34-1ff25ecca436",
   "metadata": {},
   "source": [
    "## Get and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f94434b-18eb-408a-8810-af9f27cdf522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ./data/data_quadrants.mat\n",
      "shuffling the data before train/validation/test split.\n",
      "raw_data        = (9000, 100, 100, 1)\n",
      "training data   = (7200, 100, 100, 1), (7200,)\n",
      "validation data = (1800, 100, 100, 1), (1800,)\n",
      "test data       = (0, 100, 100, 1), (0,)\n",
      "X_mean          = -8.325328964462919e-05\n",
      "X_std           = 0.1127845155013707\n"
     ]
    }
   ],
   "source": [
    "imp.reload(data_functions)\n",
    "DATA_NAME = settings['data_name']\n",
    "DATA_DIR = settings['data_dir']\n",
    "\n",
    "if(EXP_NAME[:3]=='mjo'):\n",
    "\n",
    "    labels, data, lat, lon, time = data_functions.load_mjo_data(DATA_DIR)\n",
    "    X_train, y_train, time_train, X_val, y_val, time_val, X_test, y_test, time_test = data_functions.get_and_process_mjo_data(labels,\n",
    "                                                                                         data,\n",
    "                                                                                         time,\n",
    "                                                                                         rng, \n",
    "                                                                                         colored=settings['colored'],\n",
    "                                                                                         standardize=settings['standardize'],\n",
    "                                                                                         shuffle=settings['shuffle'],\n",
    "                                                                                        )        \n",
    "elif(EXP_NAME[:8]=='extremes'):\n",
    "    filename = DATA_DIR + DATA_NAME + '.mat'\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, lat, lon = data_functions.get_and_process_data(filename, \n",
    "                                                                                        rng, \n",
    "                                                                                        colored=settings['colored'],\n",
    "                                                                                        standardize=settings['standardize'],\n",
    "                                                                                        shuffle=settings['shuffle'],\n",
    "                                                                                        ) \n",
    "    \n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = data_functions.subsample_extremes(RANDOM_SEED, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "    \n",
    "elif(EXP_NAME[:5]=='jamin' or EXP_NAME[:9]=='quadrants'):\n",
    "    filename = DATA_DIR + DATA_NAME + '.mat'\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, lat, lon = data_functions.get_and_process_data(filename, \n",
    "                                                                                        rng, \n",
    "                                                                                        colored=settings['colored'],\n",
    "                                                                                        standardize=settings['standardize'],\n",
    "                                                                                        shuffle=settings['shuffle'],\n",
    "                                                                                        )      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b621855-61ee-4e16-9948-9f2bcf61721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_class_mask = network.createClassIdentity(PROTOTYPES_PER_CLASS)\n",
    "\n",
    "prototypes_of_correct_class_train = np.zeros((len(y_train),NPROTOTYPES))\n",
    "for i in range(0,prototypes_of_correct_class_train.shape[0]):\n",
    "    prototypes_of_correct_class_train[i,:] = proto_class_mask[:,int(y_train[i])]\n",
    "    \n",
    "prototypes_of_correct_class_val   = np.zeros((len(y_val),NPROTOTYPES))    \n",
    "for i in range(0,prototypes_of_correct_class_val.shape[0]):\n",
    "    prototypes_of_correct_class_val[i,:] = proto_class_mask[:,int(y_val[i])]\n",
    "\n",
    "prototypes_of_correct_class_test   = np.zeros((len(y_test),NPROTOTYPES))    \n",
    "for i in range(0,prototypes_of_correct_class_test.shape[0]):\n",
    "    prototypes_of_correct_class_test[i,:] = proto_class_mask[:,int(y_test[i])]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b2787-9d90-4955-90a4-1a6526c918e9",
   "metadata": {},
   "source": [
    "## Define the training callbacks and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ca24eb-ab9c-4db0-82c8-94980521c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < LR_CALLBACK_EPOCH:\n",
    "        return np.round(lr,8)\n",
    "    else:\n",
    "        if(epoch % 2 == 0):\n",
    "            return lr/2.\n",
    "        else:\n",
    "            return lr\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)    \n",
    "    \n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_sparse_categorical_accuracy', \n",
    "    mode='max',\n",
    "    patience=settings['patience'], \n",
    "    restore_best_weights=True, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_list = [\n",
    "#     lr_callback,\n",
    "#     es_callback,\n",
    "]            \n",
    "\n",
    "# metrics\n",
    "metrics_list = [\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa63e6-adc0-40b0-92a1-16dd4cc806f6",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87cc2fb-5cf9-4029-824c-c24262ccb729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[32, 32]\n",
      "(100, 100, 1)\n",
      "3\n",
      "[5, 5, 5]\n",
      "30\n",
      "False\n",
      "0.17197201619672103\n",
      "-0.017197201619672104\n",
      "0.5\n",
      "-0.5\n",
      "False\n",
      "8\n",
      "128\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "False\n",
      "Model: \"full_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 100, 100, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv2D)                 (None, 100, 100, 32) 320         inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 100, 100, 32) 0           conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_0 (AveragePooling2D) (None, 50, 50, 32)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 50, 50, 32)   9248        maxpooling_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 50, 32)   0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_1 (AveragePooling2D) (None, 25, 25, 32)   0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "first_1x1_conv (Conv2D)         (None, 25, 25, 128)  4224        maxpooling_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "second_1x1_conv (Conv2D)        (None, 25, 25, 128)  16512       first_1x1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "prototypes_of_correct_class (In [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "prototype (Prototype)           (None, 15)           11295       second_1x1_conv[0][0]            \n",
      "                                                                 prototypes_of_correct_class[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "final_weights (FinalWeights)    (None, 3)            45          prototype[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Softmax)        (None, 3)            0           final_weights[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 41,644\n",
      "Trainable params: 41,644\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "__ = imp.reload(network)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = network.build_model(\n",
    "    nlayers              = NLAYERS,\n",
    "    nfilters             = NFILTERS,\n",
    "    input_shape          = X_train.shape[1:],\n",
    "    output_shape         = NCLASSES,\n",
    "    prototypes_per_class = PROTOTYPES_PER_CLASS,\n",
    "    network_seed         = RANDOM_SEED,    \n",
    "    prototype_channels   = settings['prototype_channels'],    \n",
    "    coeff_cluster        = settings['coeff_cluster'],\n",
    "    coeff_separation     = settings['coeff_separation'],\n",
    "    coeff_l1             = settings['coeff_l1'],\n",
    "    incorrect_strength   = settings['incorrect_strength'],\n",
    "    double_conv          = settings['double_conv'],\n",
    "    kernel_l1_coeff      = 0.0,#settings['kernel_l1_coeff'],\n",
    "    kernel_l2_coeff      = 0.0,#settings['kernel_l2_coeff'],\n",
    "    drop_rate            = 0.0,\n",
    "    drop_rate_final      = 0.0,        \n",
    "    \n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404175a3-bc7c-4f2e-8445-3985983c8458",
   "metadata": {},
   "source": [
    "## Load pre-trained weights into convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8dbbe1c-cf25-489c-898a-905a537609af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained convolutional layers from ./saved_models/quadrants/pretrained_model_quadrants\n",
      "   loading pretrained weights for --> conv_0\n",
      "   loading pretrained weights for --> conv_1\n"
     ]
    }
   ],
   "source": [
    "if(settings['pretrain'] == True):\n",
    "\n",
    "    if(settings['pretrain_exp'] is None):\n",
    "        PRETRAINED_MODEL = model_dir + 'pretrained_model_' + EXP_NAME \n",
    "    else:\n",
    "        PRETRAINED_MODEL = './saved_models/' + settings['pretrain_exp'] \n",
    "    \n",
    "    print('loading pretrained convolutional layers from ' + PRETRAINED_MODEL)\n",
    "    pretrained_model = tf.keras.models.load_model(PRETRAINED_MODEL)\n",
    "\n",
    "    for layer in range(1,len(model.layers)):\n",
    "        if(model.layers[layer].name[:4]=='conv'):\n",
    "            print('   loading pretrained weights for --> ' + model.layers[layer].name)\n",
    "            model.layers[layer].set_weights(pretrained_model.layers[layer].get_weights())\n",
    "else:\n",
    "    print('no pretrained model specified. keeping random initialized weights.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e13996-3403-43f2-9544-173155b4ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise ValueError('here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5c00a-658f-4597-a2fe-57e838c1db8d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b26f9-6628-4c46-b1d3-dda086aa468f",
   "metadata": {},
   "source": [
    "# Run Training Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab06f5d-7264-4ec3-ba4b-b29712e442e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(network)\n",
    "imp.reload(plots)\n",
    "imp.reload(push_prototypes)\n",
    "imp.reload(experiment_settings)\n",
    "settings = experiment_settings.get_settings(EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e32234e-5283-48e5-8024-d1fd279e80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| np.shape(X_train): (7200, 100, 100, 1)\n",
      "ic| np.shape(prototypes_of_correct_class_train): (7200, 15)\n",
      "ic| np.shape(prototypes_of_correct_class_train): (7200, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7200, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(np.shape(X_train))\n",
    "ic(np.shape(prototypes_of_correct_class_train))\n",
    "ic(np.shape(prototypes_of_correct_class_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b03cd-4810-4c70-b8fa-3fb05caa3e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| np.min(model.layers[-3].get_weights()[1]): 0.0\n",
      "    np.max(model.layers[-3].get_weights()[1]): 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "TRAINING STAGE = 0\n",
      "--------------------\n",
      "   conv_0 --> False\n",
      "   maxpooling_0 --> False\n",
      "   conv_1 --> False\n",
      "   maxpooling_1 --> False\n",
      "   first_1x1_conv --> True\n",
      "   second_1x1_conv --> True\n",
      "   prototype --> True\n",
      "   final_weights --> False\n",
      "learning rate = 0.01\n",
      "Training the model...\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - 24s 103ms/step - loss: 8.5385 - sparse_categorical_accuracy: 0.6297 - cluster_cost: 1.6008 - separation_cost: 1.5408 - l1_weights_cost: 7.5000 - val_loss: 7.9607 - val_sparse_categorical_accuracy: 0.8406 - val_cluster_cost: 0.3339 - val_separation_cost: 0.2886 - val_l1_weights_cost: 7.5000\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 23s 104ms/step - loss: 7.8147 - sparse_categorical_accuracy: 0.8975 - cluster_cost: 0.2670 - separation_cost: 0.3340 - l1_weights_cost: 7.5000 - val_loss: 7.7566 - val_sparse_categorical_accuracy: 0.9328 - val_cluster_cost: 0.2370 - val_separation_cost: 0.3193 - val_l1_weights_cost: 7.5000\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 24s 108ms/step - loss: 7.7022 - sparse_categorical_accuracy: 0.9407 - cluster_cost: 0.1938 - separation_cost: 0.2561 - l1_weights_cost: 7.5000 - val_loss: 7.6770 - val_sparse_categorical_accuracy: 0.9556 - val_cluster_cost: 0.2491 - val_separation_cost: 0.3127 - val_l1_weights_cost: 7.5000\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 24s 108ms/step - loss: 7.6577 - sparse_categorical_accuracy: 0.9525 - cluster_cost: 0.1552 - separation_cost: 0.1991 - l1_weights_cost: 7.5000 - val_loss: 7.6399 - val_sparse_categorical_accuracy: 0.9639 - val_cluster_cost: 0.2100 - val_separation_cost: 0.2446 - val_l1_weights_cost: 7.5000\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 23s 100ms/step - loss: 7.6507 - sparse_categorical_accuracy: 0.9544 - cluster_cost: 0.1560 - separation_cost: 0.2030 - l1_weights_cost: 7.5000 - val_loss: 7.6280 - val_sparse_categorical_accuracy: 0.9622 - val_cluster_cost: 0.1522 - val_separation_cost: 0.1838 - val_l1_weights_cost: 7.5000\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 22s 97ms/step - loss: 7.6379 - sparse_categorical_accuracy: 0.9610 - cluster_cost: 0.1473 - separation_cost: 0.1872 - l1_weights_cost: 7.5000 - val_loss: 7.6153 - val_sparse_categorical_accuracy: 0.9672 - val_cluster_cost: 0.1246 - val_separation_cost: 0.1672 - val_l1_weights_cost: 7.5000\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 25s 112ms/step - loss: 7.6305 - sparse_categorical_accuracy: 0.9629 - cluster_cost: 0.1504 - separation_cost: 0.1883 - l1_weights_cost: 7.5000 - val_loss: 7.6082 - val_sparse_categorical_accuracy: 0.9689 - val_cluster_cost: 0.1434 - val_separation_cost: 0.1928 - val_l1_weights_cost: 7.5000\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 23s 101ms/step - loss: 7.6239 - sparse_categorical_accuracy: 0.9624 - cluster_cost: 0.1394 - separation_cost: 0.1852 - l1_weights_cost: 7.5000 - val_loss: 7.6169 - val_sparse_categorical_accuracy: 0.9633 - val_cluster_cost: 0.1490 - val_separation_cost: 0.1927 - val_l1_weights_cost: 7.5000\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 23s 100ms/step - loss: 7.6240 - sparse_categorical_accuracy: 0.9650 - cluster_cost: 0.1451 - separation_cost: 0.1883 - l1_weights_cost: 7.5000 - val_loss: 7.6156 - val_sparse_categorical_accuracy: 0.9656 - val_cluster_cost: 0.1306 - val_separation_cost: 0.1890 - val_l1_weights_cost: 7.5000\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 27s 120ms/step - loss: 7.6212 - sparse_categorical_accuracy: 0.9660 - cluster_cost: 0.1427 - separation_cost: 0.1808 - l1_weights_cost: 7.5000 - val_loss: 7.6392 - val_sparse_categorical_accuracy: 0.9600 - val_cluster_cost: 0.1744 - val_separation_cost: 0.2180 - val_l1_weights_cost: 7.5000\n",
      "Training complete.\n",
      "\n",
      "saving model and weights to ./saved_models/quadrants/model_quadrants_stage0\n",
      "INFO:tensorflow:Assets written to: ./saved_models/quadrants/model_quadrants_stage0/assets\n",
      "--------------------\n",
      "TRAINING STAGE = 1\n",
      "--------------------\n",
      "Running Prototype Push\n",
      "8/8 [==============================] - 10s 1s/step\n",
      "Performing push of prototypes.\n",
      "Push complete.\n",
      "\n",
      "   conv_0 --> False\n",
      "   maxpooling_0 --> False\n",
      "   conv_1 --> False\n",
      "   maxpooling_1 --> False\n",
      "   first_1x1_conv --> False\n",
      "   second_1x1_conv --> False\n",
      "   prototype --> False\n",
      "   final_weights --> True\n",
      "learning rate = 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| np.min(model.layers[-3].get_weights()[1]): -0.8445202\n",
      "    np.max(model.layers[-3].get_weights()[1]): 2.954657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - 14s 56ms/step - loss: 6.5382 - sparse_categorical_accuracy: 0.8658 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 2.9739 - val_loss: 1.3293 - val_sparse_categorical_accuracy: 0.8711 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.2845\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 0.9342 - sparse_categorical_accuracy: 0.8760 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0784 - val_loss: 0.8732 - val_sparse_categorical_accuracy: 0.8722 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0495\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 0.7067 - sparse_categorical_accuracy: 0.8854 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0476 - val_loss: 0.6504 - val_sparse_categorical_accuracy: 0.8844 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0475\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 0.5601 - sparse_categorical_accuracy: 0.8860 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0522 - val_loss: 0.5173 - val_sparse_categorical_accuracy: 0.8878 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0618\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 0.4592 - sparse_categorical_accuracy: 0.8819 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0534 - val_loss: 0.4028 - val_sparse_categorical_accuracy: 0.8833 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0416\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 0.3834 - sparse_categorical_accuracy: 0.8849 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0455 - val_loss: 0.3516 - val_sparse_categorical_accuracy: 0.8922 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0359\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 0.3555 - sparse_categorical_accuracy: 0.8832 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0428 - val_loss: 0.3395 - val_sparse_categorical_accuracy: 0.8894 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0332\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 0.3584 - sparse_categorical_accuracy: 0.8853 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0455 - val_loss: 0.3515 - val_sparse_categorical_accuracy: 0.8944 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0423\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 0.3626 - sparse_categorical_accuracy: 0.8804 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0477 - val_loss: 0.3573 - val_sparse_categorical_accuracy: 0.8789 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0530\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 0.3674 - sparse_categorical_accuracy: 0.8783 - cluster_cost: 0.0026 - separation_cost: 6.9987e-04 - l1_weights_cost: 0.0508 - val_loss: 0.3535 - val_sparse_categorical_accuracy: 0.8861 - val_cluster_cost: 0.0026 - val_separation_cost: 7.0833e-04 - val_l1_weights_cost: 0.0436\n",
      "Training complete.\n",
      "\n",
      "saving model and weights to ./saved_models/quadrants/model_quadrants_stage1\n",
      "INFO:tensorflow:Assets written to: ./saved_models/quadrants/model_quadrants_stage1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| np.min(model.layers[-3].get_weights()[1]): -0.8445202\n",
      "    np.max(model.layers[-3].get_weights()[1]): 2.954657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "TRAINING STAGE = 2\n",
      "--------------------\n",
      "   conv_0 --> True\n",
      "   maxpooling_0 --> True\n",
      "   conv_1 --> True\n",
      "   maxpooling_1 --> True\n",
      "   first_1x1_conv --> True\n",
      "   second_1x1_conv --> True\n",
      "   prototype --> True\n",
      "   final_weights --> False\n",
      "learning rate = 0.01\n",
      "Training the model...\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - 40s 173ms/step - loss: 0.3900 - sparse_categorical_accuracy: 0.8729 - cluster_cost: 0.0774 - separation_cost: 0.0614 - l1_weights_cost: 0.0436 - val_loss: 0.2205 - val_sparse_categorical_accuracy: 0.9333 - val_cluster_cost: 0.0294 - val_separation_cost: 0.0333 - val_l1_weights_cost: 0.0436\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 39s 174ms/step - loss: 0.2330 - sparse_categorical_accuracy: 0.9349 - cluster_cost: 0.0223 - separation_cost: 0.0245 - l1_weights_cost: 0.0436 - val_loss: 0.1627 - val_sparse_categorical_accuracy: 0.9606 - val_cluster_cost: 0.0197 - val_separation_cost: 0.0244 - val_l1_weights_cost: 0.0436\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 40s 179ms/step - loss: 0.1893 - sparse_categorical_accuracy: 0.9522 - cluster_cost: 0.0192 - separation_cost: 0.0213 - l1_weights_cost: 0.0436 - val_loss: 0.1434 - val_sparse_categorical_accuracy: 0.9694 - val_cluster_cost: 0.0147 - val_separation_cost: 0.0187 - val_l1_weights_cost: 0.0436\n",
      "Epoch 4/10\n",
      " 29/225 [==>...........................] - ETA: 33s - loss: 0.2309 - sparse_categorical_accuracy: 0.9332 - cluster_cost: 0.0217 - separation_cost: 0.0271 - l1_weights_cost: 0.0436"
     ]
    }
   ],
   "source": [
    "imp.reload(push_prototypes)\n",
    "NEPOCHS    = settings['nepochs']\n",
    "STAGE_LIST = (0,1,2,3,4,5)#(0,1,2,3,4,5)#range(len(NEPOCHS))#(1,2,3,4,5)#range(len(NEPOCHS))\n",
    "\n",
    "for stage in STAGE_LIST:\n",
    "    \n",
    "    print('--------------------')\n",
    "    print('TRAINING STAGE = ' + str(stage))\n",
    "    print('--------------------')\n",
    "\n",
    "    # load previously trained stage, unless it is the 0th stage\n",
    "    if(stage != 0):\n",
    "        tf.keras.backend.clear_session()\n",
    "        model_filename = model_dir + 'model_' + EXP_NAME + '_stage' + str(stage-1)\n",
    "#         model = common_functions.load_model(model_filename)\n",
    "        model.load_weights(model_filename)\n",
    "        \n",
    "    # learn layers (during even numbered stages)\n",
    "    if(stage % 2 == 0):\n",
    "        # train prototypes layers (and possibly CNN layers)\n",
    "        if(settings['pretrain']==False and settings['train_cnn_in_stage'] == True):\n",
    "            model = network.set_trainable_layers(model, [True,True,True,False])            \n",
    "        elif(settings['train_cnn_in_stage'] == False or stage==0):\n",
    "            model = network.set_trainable_layers(model, [False,True,True,False])\n",
    "        elif(settings['train_cnn_in_stage'] == True):\n",
    "            model = network.set_trainable_layers(model, [True,True,True,False])            \n",
    "        elif(stage >= settings['train_cnn_in_stage']):\n",
    "            model = network.set_trainable_layers(model, [True,True,True,False])            \n",
    "        else:\n",
    "            model = network.set_trainable_layers(model, [False,True,True,False])\n",
    "    else:\n",
    "        #.......................................................\n",
    "        # push the prototypes\n",
    "        #.......................................................        \n",
    "        model, push_info = push_prototypes.push(model, \n",
    "                                                [X_train,prototypes_of_correct_class_train], \n",
    "                                                prototypes_of_correct_class_train, \n",
    "                                                perform_push=True,\n",
    "                                                batch_size=BATCH_SIZE_PREDICT,\n",
    "                                                verbose=False,\n",
    "                                               )        \n",
    "        print('Push complete.\\n')            \n",
    "\n",
    "        # train weights layer only\n",
    "        model = network.set_trainable_layers(model, [False,False,False,True])        \n",
    "\n",
    "    #.......................................................\n",
    "    # compile the model\n",
    "    #.......................................................\n",
    "    if(stage>=settings['cut_lr_stage']):\n",
    "        lr_factor = 10.**(np.floor((stage-settings['cut_lr_stage']+2)/2))\n",
    "    else:\n",
    "        lr_factor = 1.\n",
    "    if(LR_INIT/lr_factor<settings['min_lr']):\n",
    "        lr_factor = LR_INIT/settings['min_lr']\n",
    "    print('learning rate = ' + str(np.asarray(LR_INIT/lr_factor,dtype='float32')))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=np.asarray(LR_INIT/lr_factor,dtype='float32'), \n",
    "        ),\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics = metrics_list,\n",
    "    )\n",
    "#     model.summary()\n",
    "    ic(np.min(model.layers[-3].get_weights()[1]),np.max(model.layers[-3].get_weights()[1]))\n",
    "\n",
    "    #.......................................................\n",
    "    # train the model\n",
    "    #.......................................................\n",
    "    print('Training the model...')    \n",
    "    \n",
    "    tf.random.set_seed(RANDOM_SEED)   \n",
    "    np.random.seed(RANDOM_SEED)    \n",
    "    history = model.fit(\n",
    "        [X_train,prototypes_of_correct_class_train],\n",
    "        y_train,\n",
    "        validation_data=([[X_val,prototypes_of_correct_class_val]], [y_val]),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NEPOCHS[stage],\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "    print('Training complete.\\n')            \n",
    "        \n",
    "\n",
    "    # save the model at this training stage\n",
    "    model_filename = model_dir + 'model_' + EXP_NAME + '_stage' + str(stage)\n",
    "    common_functions.save_model(model, model_filename) \n",
    "    \n",
    "    #.......................................................\n",
    "    # plot results\n",
    "    #.......................................................  \n",
    "    try:\n",
    "        # plot loss history of the model\n",
    "        plots.plot_loss_history(history)\n",
    "        plt.savefig(model_diagnostics_dir + EXP_NAME + '_loss_history_stage' + str(stage) + '.png', dpi=dpiFig)    \n",
    "        plt.close()\n",
    "\n",
    "        # plot the weights\n",
    "        plots.plot_weights(model, PROTOTYPES_PER_CLASS)    \n",
    "        plt.savefig(model_diagnostics_dir + EXP_NAME + '_weights_stage' + str(stage) + '.png', dpi=dpiFig)\n",
    "        plt.close()\n",
    "    except:\n",
    "        print('not making plots...')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b663b0-f667-4172-a71a-7344b14b9590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
